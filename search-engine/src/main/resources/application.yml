spring:
  application:
    name: distributed-crawler-indexer
  
  main:
    allow-bean-definition-overriding: true

  # Database Configuration (PostgreSQL)
  datasource:
    url: jdbc:postgresql://localhost:5432/crawler_db
    username: crawler_user
    password: crawler_pass
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000

  # JPA Configuration
  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    hibernate:
      ddl-auto: none # Let Flyway handle schema
    show-sql: false
    properties:
      hibernate:
        format_sql: true
        jdbc:
          batch_size: 50
        order_inserts: true
        order_updates: true

  # Flyway Configuration
  flyway:
    enabled: true
    baseline-on-migrate: true
    locations: classpath:db/migration
    schemas: public

  # Kafka Configuration
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      acks: all
      retries: 3
      compression-type: snappy
    consumer:
      group-id: crawler-consumer-group
      auto-offset-reset: earliest
      enable-auto-commit: false
      max-poll-records: 50

  # Redis Configuration
  data:
    redis:
      host: localhost
      port: 6379
      timeout: 2000
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 2

  # Elasticsearch Configuration
  elasticsearch:
    uris: http://localhost:9200
    connection-timeout: 5s
    socket-timeout: 30s

# Server Configuration
server:
  port: 8080
  compression:
    enabled: true
  http2:
    enabled: true

# Management Endpoints (Actuator)
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  metrics:
    export:
      prometheus:
        enabled: true

# Application-Specific Configuration
crawler:
  batch-size: 100
  max-depth: 3
  threads:
    core-pool-size: 20
    max-pool-size: 100
  retry:
    max-attempts: 3
    backoff-delay: 5000
  rate-limit:
    default-delay-ms: 1000
    max-concurrent-per-domain: 5

indexer:
  batch-size: 50
  max-tokens: 10000
  pagerank:
    update-interval: 604800000 # 1 week in milliseconds

search:
  cache:
    ttl-minutes: 30
  default-page-size: 10
  max-page-size: 100

# Logging Configuration
logging:
  level:
    root: INFO
    com.searchengine: DEBUG
    org.springframework.kafka: WARN
    org.elasticsearch: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: logs/crawler.log
    max-size: 100MB
    max-history: 30